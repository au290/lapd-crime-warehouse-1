version: '3.8'

# --- DEFINISI REUSABLE ---
x-airflow-common: &airflow-common
  build:
    context: ./docker/airflow
    dockerfile: Dockerfile
  environment:
    &airflow-common-env
    AIRFLOW__CORE__EXECUTOR: LocalExecutor
    AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
    AIRFLOW__CORE__FERNET_KEY: ''
    AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
    AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
    AIRFLOW__WEBSERVER__SECRET_KEY: 'kunci_rahasia_bebas_apa_saja'
    # [NEW] Connection string for the Data Warehouse
    WAREHOUSE_CONN: postgresql+psycopg2://admin:admin_password@warehouse:5432/lapd_warehouse
  volumes:
    - ./dags:/opt/airflow/dags
    - ./src:/opt/airflow/src
    - ./plugins:/opt/airflow/plugins
    - ./config:/opt/airflow/config
    - ./scripts:/opt/airflow/scripts
    - ./governance:/opt/airflow/governance
  user: "${AIRFLOW_UID:-50000}:0"
  depends_on:
    - postgres
    - warehouse

services:
  # --- 1. METADATA DB (For Airflow Internal Use Only) ---
  postgres:
    image: postgres:13
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    volumes:
      - postgres-db-volume:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "airflow"]
      interval: 5s
      retries: 5

  # --- 2. DATA WAREHOUSE (The New Central Storage) ---
  # Replaces MinIO. Using a separate Postgres instance.
  # ... inside docker-compose.yaml ...
  warehouse:
    image: postgres:15
    environment:
      POSTGRES_USER: admin
      POSTGRES_PASSWORD: admin_password
      POSTGRES_DB: lapd_warehouse
    ports:
      - "5439:5432"
    volumes:
      - warehouse-data:/var/lib/postgresql/data
      # [ADD THIS LINE] This executes any .sql file in this folder on startup
      - ./docker/postgres:/docker-entrypoint-initdb.d
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U admin -d lapd_warehouse"]
      interval: 10s
      retries: 5

  # --- 3. DASHBOARD (STREAMLIT) ---
  dashboard:
    build:
      context: ./docker/airflow
      dockerfile: Dockerfile
    entrypoint: /bin/bash
    # [FIX] Added --server.address 0.0.0.0 so it is accessible from host
    command: -c "streamlit run /opt/airflow/src/dashboard/app.py --server.port 8501 --server.address 0.0.0.0"
    ports:
      - "8501:8501"
    environment:
      # Dashboard needs to know how to connect to the Warehouse
      WAREHOUSE_CONN: postgresql+psycopg2://admin:admin_password@warehouse:5432/lapd_warehouse
    volumes:
      - ./src:/opt/airflow/src
    depends_on:
      - warehouse

  # --- AIRFLOW SERVICES ---
  airflow-webserver:
    <<: *airflow-common
    command: webserver
    ports:
      - "8080:8080"
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
      interval: 10s
      timeout: 10s
      retries: 5
    depends_on:
      - airflow-scheduler

  airflow-scheduler:
    <<: *airflow-common
    command: scheduler
    hostname: airflow-scheduler
    depends_on:
      - postgres
      - warehouse

  airflow-init:
    <<: *airflow-common
    command: version
    environment:
      <<: *airflow-common-env
      _AIRFLOW_DB_UPGRADE: 'true'
      _AIRFLOW_WWW_USER_CREATE: 'true'
      _AIRFLOW_WWW_USER_USERNAME: 'admin'
      _AIRFLOW_WWW_USER_PASSWORD: 'admin'
    user: "0:0"

volumes:
  postgres-db-volume:
  warehouse-data: